{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSYvP0n8RjYH"
   },
   "source": [
    "# Lab01: Frequent itemset mining\n",
    "\n",
    "- Student ID: 18127070 \n",
    "- Student name: Trần Đại Chí\n",
    "\n",
    "**How to do your homework**\n",
    "\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
    "\n",
    "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
    "\n",
    "**How to submit your homework**\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
    "\n",
    "Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`) Copy file notebook to this folder, compress and submit it on moodle.\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "- Frequent itemset mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXZ5gCVaRjYa"
   },
   "source": [
    "# 1. Preliminaries\n",
    "## This is how it all started ...\n",
    "- Rakesh Agrawal, Tomasz Imielinski, Arun N. Swami: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Conference 1993: 207-216\n",
    "- Rakesh Agrawal, Ramakrishnan Srikant: Fast Algorithms for Mining Association Rules in Large Databases. VLDB 1994: 487-499\n",
    "\n",
    "**These two papers are credited with the birth of Data Mining**\n",
    "## Frequent itemset mining (FIM)\n",
    "\n",
    "Find combinations of items (itemsets) that occur frequently.\n",
    "## Applications\n",
    "- Items = products, transactions = sets of products someone bought in one trip to the store.\n",
    "$\\Rightarrow$ items people frequently buy together.\n",
    "    + Example: if people usually buy bread and coffee together, we run a sale of bread to attract people attention and raise price of coffee.\n",
    "- Items = webpages, transactions = words. Unusual words appearing together in a large number of documents, e.g., “Brad” and “Angelina,” may indicate an interesting relationship.\n",
    "- Transactions = Sentences, Items = Documents containing those sentences. Items that appear together too often could represent plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8vAJ8A2RjYi"
   },
   "source": [
    "## Transactional Database\n",
    "A transactional database $D$ consists of $N$ transactions: $D=\\left\\{T_1,T_2,...,T_N\\right\\}$. A transaction $T_n \\in D (1 \\le n \\le N)$ contains one or more items and that $I= \\left\\{ i_1,i_2,…,i_M \\right\\}$ is the set of distinct items in $D$, $T_n \\subset I$. Commonly, a transactional database is represented by a flat file instead of a database system: items are non-negative integers, each row represents a transaction, items in a transaction separated by space.\n",
    "\n",
    "Example: \n",
    "\n",
    "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
    "\n",
    "30 31 32 \n",
    "\n",
    "33 34 35 \n",
    "\n",
    "36 37 38 39 40 41 42 43 44 45 46 \n",
    "\n",
    "38 39 47 48 \n",
    "\n",
    "38 39 48 49 50 51 52 53 54 55 56 57 58 \n",
    "\n",
    "32 41 59 60 61 62 \n",
    "\n",
    "3 39 48 \n",
    "\n",
    "63 64 65 66 67 68 \n",
    "\n",
    "\n",
    "\n",
    "# Definition\n",
    "\n",
    "- Itemset: A collection of one or more items.\n",
    "    + Example: {1 4 5}\n",
    "- **k-itemset**: An itemset that contains k items.\n",
    "- Support: Frequency of occurrence of an itemset.\n",
    "    + Example: From the example above, item 3 appear in 2 transactions so its support is 2.\n",
    "- Frequent itemset: An itemset whose support is greater than or equal to a `minsup` threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdykKxr6RjY-"
   },
   "source": [
    "# The Apriori Principle\n",
    "- If an itemset is frequent, then all of its subsets must also be frequent.\n",
    "- If an itemset is not frequent, then all of its supersets cannot be frequent.\n",
    "- The support of an itemset never exceeds the support of its subsets.\n",
    "$$ \\forall{X,Y}: (X \\subseteq Y) \\Rightarrow s(X)\\ge s(Y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvfMR7-CRjZB"
   },
   "source": [
    "# 2. Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9gZh4DORjZD"
   },
   "source": [
    "## The Apriori algorithm\n",
    "Suppose:\n",
    "\n",
    "$C_k$ candidate itemsets of size k.\n",
    "\n",
    "$L_k$ frequent itemsets of size k.\n",
    "\n",
    "The level-wise approach of Apriori algorithm can be descibed as follow:\n",
    "1. k=1, $C_k$ = all items.\n",
    "2. While $C_k$ not empty:\n",
    "    3. Scan the database to find which itemsets in $C_k$ are frequent and put them into $L_k$.\n",
    "    4. Use $L_k$ to generate a collection of candidate itemsets $C_{k+1}$ of size k+1.\n",
    "    5. k=k+1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF9xHOBLRjZJ"
   },
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7F0lUOSuRjZN"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OogwdcLRjZf"
   },
   "source": [
    "### Read data\n",
    "First we have to read data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U2bsGrTERjZg"
   },
   "outputs": [],
   "source": [
    "\n",
    "def readData(path):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------\n",
    "        path: path of database D.\n",
    "         \n",
    "    --------------------------\n",
    "    Returns\n",
    "        data: a dictionary for representing database D\n",
    "                 - keys: transaction tids\n",
    "                 - values: itemsets.\n",
    "        s: support of distict items in D.\n",
    "    \"\"\"\n",
    "    data={}\n",
    "    s=defaultdict(lambda: 0) # Initialize a dictionary for storing support of items in I.  \n",
    "    with open(path,'rt') as f:\n",
    "        tid=1;\n",
    "        for line in f:\n",
    "            itemset=set(map(int,line.split())) # a python set is a native way for storing an itemset.\n",
    "            for item in itemset:  \n",
    "                s[item]+=1     #Why don't we compute support of items while reading data?\n",
    "            data[tid]= itemset\n",
    "            tid+=1\n",
    "    return data, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSTC78WURjZu"
   },
   "source": [
    "### FP - Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGAkmuXtRjZw"
   },
   "source": [
    "**I gave you pseudo code of Apriori algorithm above but we implement FP-Growth. Tell me the differences of two algorithms.**\n",
    "\n",
    "\n",
    "**TODO:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BVRT5BnWRjZz"
   },
   "outputs": [],
   "source": [
    "def joinset(a,b):\n",
    "    '''\n",
    "    Parameters\n",
    "    -------------------\n",
    "        2 itemsets a and b (of course they are at same branch in search space) \n",
    "\n",
    "    -------------------\n",
    "    return \n",
    "        ret: itemset generated by joining a and b\n",
    "    '''\n",
    "    #TODO (hint: this function will be called in generateSearchSpace method.):\n",
    "    ret = list(set(a) | set(b))\n",
    "    return ret\n",
    "    \n",
    "class FP_Growth:\n",
    "    def __init__ (self, data=None, s=None, minSup=None):\n",
    "        self.data=data\n",
    "        self.s={}\n",
    "        \n",
    "        \n",
    "        for key, support in sorted(s.items(),key= lambda item: item[1]):\n",
    "            self.s[key]=support\n",
    "        #TODO: why should we do this, answer it at the markdown below?\n",
    "           \n",
    "        self.minSup=minSup\n",
    "        self.L={}  #Store frequent itemsets mined from database\n",
    "        self.runAlgorithm()\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Initialize search space at first step\n",
    "        --------------------------------------\n",
    "        We represent our search space in a tree structure\n",
    "        \"\"\"\n",
    "        tree={}\n",
    "\n",
    "        search_space={}  \n",
    "        for item, support in self.s.items():\n",
    "            search_space[item]={}\n",
    "            \n",
    "            search_space[item]['itemset']=[item] \n",
    "            ''' \n",
    "            python set does not remain elements order\n",
    "            so we use a list to extend it easily when create new itemset \n",
    "            but why we store itemset in data by a python set???? '''\n",
    "            # TODO: study about python set and its advantages,\n",
    "            # answer at the markdown below.\n",
    "            \n",
    "            search_space[item]['pruned']=False\n",
    "            # TODO:\n",
    "            # After finish implementing the algorithm tell me why should you use this \n",
    "            # instead of delete item directly from search_space and tree. \n",
    "            \n",
    "            search_space[item]['support']=support\n",
    "            \n",
    "            tree[item]={}\n",
    "            '''\n",
    "            Why should i store an additional tree (here it called tree)? \n",
    "            Answer: This really help in next steps.\n",
    "            \n",
    "            Remember that there is always a big gap from theory to practicality\n",
    "            and implementing this algorithm in python is not as simple as you think.\n",
    "            ''' \n",
    "        \n",
    "        return tree, search_space\n",
    "    \n",
    "    def computeItemsetSupport(self, itemset):\n",
    "        '''Return support of itemset'''\n",
    "        #TODO (hint: this is why i use python set in data)\n",
    "        support = self.s[itemset] / len(self.data)\n",
    "        return support\n",
    "    \n",
    "    \n",
    "        \n",
    "    def prune(self,k, tree, search_space):\n",
    "        '''\n",
    "        In this method we will find out which itemset in current search space is frequent \n",
    "        itemset then add it to L[k]. In addition, we prune those are not frequent itemsets. \n",
    "        '''\n",
    "        #TODO\n",
    "        items = list(tree.keys())\n",
    "        for item in items:\n",
    "            support = self.computeItemsetSupport(item)\n",
    "            compute_sup = self.minSup / len(self.data)\n",
    "            if support >= compute_sup:\n",
    "                self.L[k].append(item)\n",
    "                search_space[item]['pruned'] = False\n",
    "            else:\n",
    "                self.L[k] = []\n",
    "                search_space[item]['pruned'] = True  \n",
    "        \n",
    "        \n",
    "                \n",
    "    def generateSearchSpace(self,k, tree, search_space):\n",
    "        if k <= 0: return\n",
    "        '''\n",
    "        Generate search space for exploring k+1 itemset. (Recursive function) \n",
    "        '''\n",
    "        \n",
    "        items=list(tree.keys())  \n",
    "        ''' print search_space.keys() you will understand  \n",
    "         why we need an additional tree, '''\n",
    "         \n",
    "    \n",
    "        l=len(items)\n",
    "        self.prune(k, tree, search_space)\n",
    "        if l==0: return   #Stop condition\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(l-1):\n",
    "            a=items[i]\n",
    "            if search_space[a]['pruned']: continue \n",
    "                \n",
    "            for j in range(i+1,l):\n",
    "                b=items[j]\n",
    "                \n",
    "                search_space[a][b]={}\n",
    "                tree[a][b]={}\n",
    "                # You really need to understand what am i doing here before doing work below.\n",
    "                # (Hint: draw tree and search space to draft). \n",
    "                \n",
    "                #TODO:\n",
    "                #First create newset using join set\n",
    "                new_l1, new_l2 = [], []\n",
    "                new_l1.append(a)\n",
    "                new_l2.append(b)\n",
    "                new_set = joinset(new_l1, new_l2)\n",
    "                new_set = set(new_set)\n",
    "                \n",
    "                #Second add newset to search_space \n",
    "                search_space[a][b] = new_set\n",
    "                \n",
    "            #  Generate search_space for k+1-itemset\n",
    "            self.generateSearchSpace(k - 1, tree, search_space)\n",
    "    \n",
    "    def runAlgorithm(self):\n",
    "        tree,search_space=self.initialize() #generate search space for 1-itemset\n",
    "        self.generateSearchSpace(1, tree, search_space)\n",
    "    def miningResults(self):\n",
    "        print(\"Frequent itemset L1\\n\")\n",
    "        return self.L\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tMTpwxLRjZ-"
   },
   "source": [
    "Ok, let's test FP-growth on a typical dataset `chess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gLygYqiYRjZ-",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data, s= readData('chess.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PnxbU77YRjaF",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent itemset L1\n",
      "\n",
      "{1: [48, 56, 66, 34, 62, 7, 36, 60, 40, 29, 52, 58]}\n"
     ]
    }
   ],
   "source": [
    "a=FP_Growth(data=data,s=s, minSup=3000)\n",
    "print(a.miningResults())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp0RFbw-RjaU"
   },
   "source": [
    "### Answer questions here:\n",
    "1) \n",
    "- self.s[key] = support, we do this to get support corresponds to each itemset in our data\n",
    "\n",
    "2) \n",
    "- we use set instead of list because in step joinset if we want to join a and b without duplicate we need to use set,\n",
    "- example a = ABC, b = CEF, a | b = ABCEF and the major advantage of using a set, as opposed to a list is that it has a highly\n",
    "- optimized method for checking whether a specific element is contained in the set and if our data is big enough, a set\n",
    "- is faster for containing checks than list\n",
    "\n",
    "3) \n",
    "- As we mentioned above, a set is faster than a list when checking if an item is contained within it, so we will check \n",
    "- rapidly itemsets that wasn't pruned "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnVm8wYIRjaV"
   },
   "source": [
    "# 3. Churn analysis\n",
    "\n",
    "In this section, you will use frequent itemset mining technique to analyze `churn` dataset (for any purposes). You can download dataset from here: http://ce.sharif.edu/courses/85-86/1/ce925/assignments/files/assignDir2/churn.txt. Write your report and implementation below.\n",
    "\n",
    "*Remember this dataset is not represented as a transactional database, first thing that you have to do is transforming it into a flat file.  \n",
    "More information about `churn` here: http://ce.sharif.edu/courses/85-86/1/ce925/assignments/files/assignDir4/Churn.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9U08alVRjaW"
   },
   "source": [
    "**TODO:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'VMail Message: 0', 'Churn?: False.\\n'} with support: 0.6\n",
      "{'VMail Plan: no', 'Churn?: False.\\n'} with support: 0.6\n",
      "{'VMail Plan: no', 'VMail Message: 0', 'Churn?: False.\\n'} with support: 0.6\n",
      "{\"Int'l Plan: no\", 'VMail Message: 0'} with support: 0.65\n",
      "{\"Int'l Plan: no\", 'VMail Plan: no'} with support: 0.65\n",
      "{\"Int'l Plan: no\", 'VMail Plan: no', 'VMail Message: 0'} with support: 0.65\n",
      "{'VMail Message: 0'} with support: 0.72\n",
      "{'VMail Plan: no'} with support: 0.72\n",
      "{'VMail Plan: no', 'VMail Message: 0'} with support: 0.72\n",
      "{\"Int'l Plan: no\", 'Churn?: False.\\n'} with support: 0.8\n",
      "{'Churn?: False.\\n'} with support: 0.85\n",
      "{\"Int'l Plan: no\"} with support: 0.9\n"
     ]
    }
   ],
   "source": [
    "def readFile():\n",
    "    f = open(\"churn.txt\", 'r')\n",
    "    for line in f:\n",
    "        data = line.split(',')\n",
    "        data[0] = \"State: \" + data[0]\n",
    "        data[1] = \"Account Length: \" + data[1]\n",
    "        data[2] = \"Area Code: \" + data[2]\n",
    "        data[3] = \"Phone: \" + data[3]\n",
    "        data[4] = \"Int'l Plan: \" + data[4]\n",
    "        data[5] = \"VMail Plan: \" + data[5]\n",
    "        data[6] = \"VMail Message: \" + data[6]\n",
    "        data[7] = \"Day Mins: \" + data[7]\n",
    "        data[8] = \"Day Calls: \" + data[8]\n",
    "        data[9] = \"Day Charge: \" + data[9]\n",
    "        data[10] = \"Eve Mins: \" + data[10]\n",
    "        data[11] = \"Eve Calls: \" + data[11]\n",
    "        data[12] = \"Eve Charge: \" + data[12]\n",
    "        data[13] = \"Night Mins: \" + data[13]\n",
    "        data[14] = \"Night Calls: \" + data[14]\n",
    "        data[15] = \"Night Charge: \" + data[15]\n",
    "        data[16] = \"Intl Mins: \" + data[16]\n",
    "        data[17] = \"Intl Calls: \" + data[17]\n",
    "        data[18] = \"Intl Charge: \" + data[18]\n",
    "        data[19] = \"CustServ Calls: \" + data[19]\n",
    "        data[20] = \"Churn?: \" + data[20]\n",
    "        if not data: break\n",
    "        yield data\n",
    "\n",
    "data = readFile()\n",
    "\n",
    "class ChurnAnalysis:\n",
    "    def __init__(self):\n",
    "        self.minSup = 2000 #0.6\n",
    "        self.rowTransaction = set() #all data of each row in dataset\n",
    "        self.L = defaultdict(int) #store number of a itemset for computing its support\n",
    "    def candidateC1(self):\n",
    "        itemSet = set()\n",
    "        for i in data:\n",
    "            tid = frozenset(i)\n",
    "            self.rowTransaction.add(tid)\n",
    "            for item in tid: itemSet.add(frozenset([item])) # use frozenset as a key in a dict\n",
    "        sorted(itemSet, reverse=False)\n",
    "        return itemSet\n",
    "    def itemWithMinsup(self, Ck):\n",
    "        prune = set() #store itemset satisfy minSup\n",
    "        cnt = defaultdict(int)\n",
    "        for tid in self.rowTransaction:\n",
    "            for item in Ck:\n",
    "                if item.issubset(tid):\n",
    "                    self.L[item] += 1\n",
    "                    cnt[item] += 1\n",
    "        for key, support in cnt.items():\n",
    "            sup = support / len(self.rowTransaction)\n",
    "            convertMinsup = round(self.minSup / len(self.rowTransaction), 2)\n",
    "            if sup >= convertMinsup: prune.add(key)\n",
    "            else: continue \n",
    "        return prune\n",
    "    def computeSupport(self, item):\n",
    "        return self.L.get(item) / len(self.rowTransaction)\n",
    "    def selfjoin(self, itemset):\n",
    "        Ck = set()\n",
    "        for a in itemset:\n",
    "            for b in itemset:\n",
    "                sorted(a, reverse=False)\n",
    "                sorted(b, reverse=False)\n",
    "                c = a & b \n",
    "                if(len(c) == len(a) - 1): Ck.add(a | b)\n",
    "        return Ck\n",
    "    def apriori(self):\n",
    "        aprioriGen = dict()\n",
    "        C1 = self.candidateC1()\n",
    "        Lk = self.itemWithMinsup(C1)\n",
    "        k = 2\n",
    "        if(len(Lk) == 0): return\n",
    "        while(len(Lk) != 0):\n",
    "            aprioriGen[k - 1] = Lk\n",
    "            new_join = self.selfjoin(Lk)\n",
    "            scanDB = self.itemWithMinsup(new_join)\n",
    "            Lk = scanDB\n",
    "            k += 1\n",
    "        result = []\n",
    "        for key, value in aprioriGen.items():\n",
    "            result.extend([(set(item), self.computeSupport(item)) for item in value])\n",
    "        for key, support in sorted(result, key=lambda item: item[1]):\n",
    "            print(str(key) + ' with support: ' + str(round(support, 2)))\n",
    "\n",
    "def main():\n",
    "    a = ChurnAnalysis()\n",
    "    a.apriori()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, i read and handle the file, then i use apriori algorithm for analysing churn purpose \n",
    "- candidateC1(self): in this function, we will generate all candidate C1 from out dataset. We use frozenset instead of set because we will later use these sets as the key in the dictionary, so we need to make it immutable\n",
    "- itemWithMinsup(self, Ck): in this function, we will have list frequent L1 from list of candidate C1 we created from function candidateC1(self), then we take itemsets satisfy our minSup\n",
    "- computeSupport(self, item): calculate support of a item in dataset\n",
    "- selfjoin(self, itemset): this function will take list of frequent itemset Lk to produce Ck. Example our L1 is  {a}, {b}, {c} -> {a, b}, {a, c}, {b, c} and two set are combined using union which is the \"|\" symbol\n",
    "- apriori(self): main function for running apriori algorithm and print all frequent itemsets Lk with their support that satisfy our minSup\n",
    "- As we can see from result above, we have total 12 frequent itemsets with minSup >= 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FzxGs7RRjaX"
   },
   "source": [
    "# 4 References\n",
    "http://ce.sharif.edu/courses/85-86/1/ce925/assignments/files/assignDir2/ProjectDefinition1.pdf\n",
    "\n",
    "https://cs.wmich.edu/~alfuqaha/summer14/cs6530/lectures/AssociationAnalysis-Part1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doYH4biqR_N7"
   },
   "source": [
    "Feel free to send questions to my email address: nnduc@fit.hcmus.edu.vn\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab01 - Frequent itemset mining.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
